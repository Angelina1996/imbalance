#' Rapidy Converging Gibbs algorithm.
#'
#' Allows you to treat imabalanced discrete numeric datasets by generating
#' synthetic minority examples by approximating their probability distribution.
#'
#' Aproximates minority distribution using Gibbs Sampler. Dataset must be
#' discretized and numeric. In each iteration, it builds a new sample using a
#' Markov chain. It discards first \code{burnin} iterations, and from then on,
#' it validates each \code{lag} example as a new minority example. It generates
#' \eqn{d (iterations-burnin)/lag} where \eqn{d} is minority examples number.
#'
#' @param dataset data.frame to treat. All columns, except \code{classAttr} one,
#'   have to be numeric.
#' @param iterations Integer. Number of iterations to run for each minority
#'   example.
#' @param burnin Integer. It determines how many examples generated for a given
#'   one are going to be discarded firstly. By default, 100.
#' @param lag Integer. Number of iterations between new generated example for a
#'   minority one. By default, 20.
#' @param classAttr String. Indicates the class attribute from \code{dataset}.
#'   Must exist in it.
#'
#' @return A \code{data.frame} with the same structure as \code{dataset},
#'   containing the synthetic examples generated.
#' @export
#' @examples
#' data(iris0)
#' set.seed(12345)
#'
#' # Generates new minority examples
#' newSamples <- racog(iris0, burnin = 10, iterations = 100, classAttr = "Class")
#'
racog <- function(dataset, iterations, burnin = 100, lag = 20, classAttr = "Class"){
  if(!is.data.frame(dataset))
    stop("dataset must be a data.frame")
  if(!classAttr %in% names(dataset))
    stop(paste(classAttr, "attribute not found in dataset"))
  if(any(! .colTypes(dataset, exclude = classAttr) == "numeric"))
    stop("all columns of dataset must be numeric")
  if(!is.numeric(burnin) || !is.numeric(lag) || !is.numeric(iterations) ||
     burnin < 0 || lag < 0 || iterations < 0)
    stop("burnin, lag and iterations must be positive integers")


  # Calcs minority class and instances
  minorityClass <- .whichMinorityClass(dataset, classAttr)
  minority <- dataset[dataset[, classAttr] == minorityClass,
                      names(dataset) != classAttr]

  gibbsSampler <- .makeGibbsSampler(minority)
  minority <- data.matrix(minority)
  newSamples <- data.frame(matrix(ncol = ncol(minority), nrow = 0))

  # For each minority example, create (iterations - burnin)/lag
  # new examples, approximating minority distribution with a Gibss sampler
  for(k in seq_len(iterations)){
    # Generate new sample using Gibbs Sampler
    minority <- t(apply(minority, MARGIN = 1, gibbsSampler))

    if(k > burnin && k%%lag == 0)
      newSamples <- rbind.data.frame(newSamples, minority)
  }


  # Prepare newSamples output
  .normalizeNewSamples(newSamples, minorityClass, names(minority), classAttr)
}



#' Wrapper for Rapidy Converging Gibbs algorithm.
#'
#' Generates synthetic minority examples by approximating their probability
#' distribution until sensitivity of \code{wrapper} over \code{validation}
#' cannot be further improved. Works only on discrete numeric datasets
#'
#' Until the last \code{slideWin} executions of \code{wrapper} over
#' \code{validation} dataset reach a mean sensitivity lower than
#' \code{threshold}, the algorithm keeps generating samples using Gibbs Sampler,
#' and adding misclassified samples with respect to a model generated by a
#' former train, to the train dataset. Iniial model is built on initial
#' \code{train}.
#'
#' @param train \code{data.frame}. A initial dataset to generate first model.
#'   All columns, except \code{classAttr} one, have to be numeric
#' @param validation \code{data.frame}. A dataset to compare results of
#'   consecutive classifiers. Must have the same structure of \code{train}.
#' @param wrapper An \code{S3} object. There must be a method
#'   \code{\link{trainWrapper}} implemented for the class of the object, and a
#'   \code{\link[stats]{predict}} method implemented for the class of the model
#'   returned by \code{trainWrapper}.
#' @param slideWin Number of last sensitivities to take into account to meet the
#'   stopping criteria. By default, 10.
#' @param threshold Threshold that the last \code{slideWin} sensitivities mean
#'   should reach. By default, 0.02.
#' @param classAttr String. Indicates the class attribute from \code{dataset}.
#'   Must exist in it.
#'
#' @return A \code{data.frame} with the same structure as \code{dataset},
#'   containing the synthetic examples generated.
#' @importFrom stats predict
#' @export
#'
#' @examples
#' data(haberman)
#' set.seed(12345)
#' myWrapper <- structure(list(), class="C50Wrapper")
#' trainWrapper.C50Wrapper <- function(wrapper, train, trainClass){
#'   C50::C5.0(train, trainClass)
#' }
#'
#' trainFold <- sample(1:nrow(haberman), nrow(haberman)/2, FALSE)
#' newSamples <- wracog(haberman[trainFold, ], haberman[-trainFold, ],
#'                      myWrapper, classAttr = "Class")
#'
wracog <- function(train, validation, wrapper, slideWin = 10,
                   threshold = 0.02, classAttr = "Class"){
  trainMethod <- paste("trainWrapper", class(wrapper), sep=".")
  colTypes <- sapply(train[, names(train) != classAttr], class)

  if(!is.data.frame(train) || !is.data.frame(validation) ||
     any(! names(train) %in% names(validation)) ||
     any(! names(validation) %in% names(train)))
    stop("train and validation must be data.frames with the same column names")
  if(any(! colTypes == "numeric"))
    stop("all columns of dataset must be numeric")
  if(!classAttr %in% names(train))
    stop(paste(classAttr, "attribute not found in dataset"))
  if((!is.numeric(slideWin) || !is.numeric(threshold)) ||
     slideWin < 0 || threshold <= 0 || threshold >= 1)
    stop("slideWin must be a positive integer \n  threshold must be in ]0,1[")
  if(!trainMethod %in% utils::methods(trainWrapper))
    stop(paste("There doesn't exist a method "), trainMethod)

  # Calcs minority class
  minorityClass <- .whichMinorityClass(train, classAttr)

  # Strip class column from both train and validation
  minority <- train[train[, classAttr] == minorityClass, names(train) != classAttr]
  trainClass <- train[, classAttr]
  validationClass <- validation[, classAttr]
  train <- train[, names(train) != classAttr]
  validation <- validation[, names(validation) != classAttr]

  # Wrapper for the Gibbs sampler with input and outpus as data.frame
  gibbsSampler <- .makeGibbsSampler(minority)

  # Value for lasts winSlides standard deviations
  lastSlides <- rep(Inf, slideWin)

  model <- trainWrapper(wrapper, train, trainClass)
  predictMethod <- paste("predict", class(model), sep=".")
  if(!predictMethod %in% utils::methods(predict))
    stop(paste("There must exist a method predict.class where class\n",
               " is the class of the model returned by", trainMethod))

  minority <- data.matrix(minority)
  newSamples <- data.frame(matrix(ncol = ncol(minority), nrow = 0))

  while(.naReplace(stats::sd(lastSlides), Inf) >= threshold){
    minority <- t(apply(minority, MARGIN = 1, gibbsSampler))
    prediction <- predict(model, minority)
    misclassified <- minority[prediction != minorityClass, , drop = FALSE]
    newSamples <- rbind.data.frame(newSamples, misclassified)
    train <- rbind.data.frame(train, misclassified)
    trainClass <- .appendfactor(trainClass, rep(minorityClass, nrow(misclassified)))
    model <- trainWrapper(wrapper, train, trainClass)
    prediction <- predict(model, validation)

    # Measure of the quality of the newTrain
    qMeasure <- .sensitivity(prediction, validationClass)
    lastSlides <- c(qMeasure, lastSlides)
    lastSlides <- lastSlides[1:slideWin]
  }

  .normalizeNewSamples(newSamples, minorityClass, names(minority), classAttr)
}


#' Generic methods to train classifiers
#'
#' @param wrapper the wrapper instance
#' @param train \code{data.frame} of the train dataset without the class column
#' @param trainClass a vector containing the class column for \code{train}
#'
#' @export
#'
#' @examples
#' myWrapper <- structure(list(), class="C50Wrapper")
#' trainWrapper.C50Wrapper <- function(wrapper, train, trainClass){
#'   C50::C5.0(train, trainClass)
#' }
trainWrapper <- function(wrapper, train, trainClass){
  UseMethod("trainWrapper")
}


#' Make a tree, represented as a matrix of edges nx2, directed.
#'
#' Returns a directed tree build up from an undirected one, complying with the
#' condition that each non-root node has a single parent.
#'
#' @param tree Matrix nx2 columns denoting undirected arcs of a tree.
#'
#' @return Matrix of directed arcs. Arcs are directed from the first coordinate
#'   towards second.
#' @noRd
#'
#' @examples
#' DT <- bnlearn::chow.liu(iris[, names(iris) != "Species"])$arcs
#'
#' tree <- unname(DT[ seq(1, nrow(DT), 2), ])
#' makeDirected(tree)
#'
.makeDirected <- function(tree){
  visited <- c()
  # For each arc, marks the second node as visited
  # If for a node second coordinate has already been visited, it inverts the sense
  # of the arc
  for (k in 1:nrow(tree)){
    if(tree[k,2] %in% visited){
      tree[k,] <- tree[k,c(2,1)]
    }
    visited <- c(visited, tree[k,2])
  }

  tree
}


#' Generate Gibss Sampler
#'
#' Generates Gibbs Sampler algorithm for approximate samples distribution
#' @param samples A numerical dataframe of samples whose distribution
#'   approximate
#'
#' @return GibbsSampler. A function that receives a sample, and generates a new
#'   one using the distribution
#' @noRd
.makeGibbsSampler <- function(samples){
  attrs <- names(samples)
  DT <- bnlearn::chow.liu(samples)$arcs
  # Choose only one sense for the arcs (the odd ones) and make tree directed
  edges <- unname(DT[ seq(1, nrow(DT), 2), ])
  edges <- .makeDirected(edges)
  edges <- apply(edges, MARGIN = c(1,2), function(attr){
    which(attrs == attr)
  })
  attrs <- seq_along(attrs)
  root = attrs[! attrs %in% edges[, 2]]

  # Calculate conditioned probability distributions
  # Cols are variables to which we are conditioning to
  genProbs <- function(i, j){
    contingency <- table(samples[, i], samples[, j])
    prop.table(contingency, margin = 1)
    #rowProbs <- as(rowProbs, "matrix")
    #hash::hash( as.list(as.data.frame(rowProbs)) )
    #as(rowProbs, "dgCMatrix")
  }

  # Calculate absolute probability distributions
  absoluteProbs <- apply(samples, MARGIN = 2, function(col){
    prop.table(table(col))
  })


  # Store attributes to which a given atribute is conditioned to,
  # attributes which are being conditioned by a given one,
  # and posible values for each attribute
  probDist <- lapply(attrs, function(attr){
    # Calc arcs in which attr is conditioned to other attribute
    conditioned <- which(edges[, 2] == attr)
    # Calc arcs in which attr is conditioning other attribute
    conditioning <- which(edges[, 1] == attr)
    conditionedProbs <- NULL
    conditioningProbs <- NULL
    values <- as.numeric(names(absoluteProbs[[attr]]))

    if(length(conditioned) > 0){
      conditioned <- edges[conditioned, ]
      conditionedProbs <- genProbs(conditioned[1], conditioned[2])
      conditioned <- conditioned[1]
    } else{
      conditioned <- NULL
    }

    if(length(conditioning) > 0){
      conditioning <- edges[conditioning, 2]

      conditioningProbs <- lapply(conditioning, function(conditioningAttr){
          genProbs(conditioningAttr, attr)
      })
    } else{
      conditioning <- NULL
    }

    list(conditionedProbs = conditionedProbs,
         conditioningProbs = conditioningProbs,
         absoluteProb = absoluteProbs[[attr]],
         conditioned = conditioned,
         conditioning = conditioning,
         values = values)
  })

  # Generate the Gibbs Sampler
  gibbsSampler <- function(x){
    for(attr in attrs){
      i <- probDist[[attr]]$conditioned
      first <- list()

      if(!is.null(i)){
        value <- toString(unlist(x[i]))
        first <- probDist[[attr]]$conditionedProbs[value, ]
      }

      valuesConditioning <- sapply(probDist[[attr]]$conditioning, function(i){
        toString(unlist(x[i]))
      })

      second <- mapply(function(dist, value){
          r <- dist[value, ]
          r / probDist[[attr]]$absoluteProb
      }, probDist[[attr]]$conditioningProbs, valuesConditioning)


      if(attr == root){
        probVectors <- cbind(first, second, probDist[[attr]]$absoluteProb)
      } else{
        probVectors <- cbind(first, second)
      }

      # Prob of attr. is product of probabilites from the dependence tree
      ithProb <- apply(probVectors, MARGIN = 1, function(r){
        prod(unlist(r))
      })


      # If all probabilities are zero, create vector with same probabilities
      if(!any(ithProb != 0))
        ithProb <- rep(1, length(ithProb))

      x[attr] = sample( probDist[[attr]]$values, 1, prob = ithProb )
    }
    # Return new sample
    x
  }

  # Return Gibbs Sampler
  gibbsSampler
}
