---
author: "Nacho CordÃ³n"
title: "Working with imbalanced datasets"
output: rmarkdown::pdf_document
geometry: margin=5cm
papersize: A4
vignette: >
  %\VignetteIndexEntry{Working with imbalanced dataset}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Imbalance classification problem
Let:

  * $S=\{(x_1, y_1), \ldots (x_m, y_m)\}$ be our training data for a classification problem.
  * $S^{+} = \{(x,y) \in S: y=1\}$ be the positive or minority instances.
  * $S^{-} = \{(x,y) \in S: y=-1\}$ be the negative or majority instances.

If $|S^{+}| > |S^{-}|$, classification algorithms' performances are highly hindered, specially when it comes to the positive class. Therefore, methods to improve that performance are required. 

Namely, `imbalance` package provides *oversampling* algorithms. Those family of procedures aim to generate a set $E$ of synthetic positive instances based on the training ones, so that we have a new classification problem with $\bar{S}^{+} = S^{+} \cup E$, $\bar{S}^{-} = S^{-}$ and $\bar{S} = \bar{S}^{+}\cup \bar{S}^{-}$ our new training set.

# Contents of the package

In the package, we have the following *oversampling* functions available:

* `mwmote`
* `racog`
* `wracog`
* `rwo`
* `pdfos`

Each of these functions can be applied to the data included in the package, which are imbalanced datasets. For example, we can run `pdfos` algorithm on `newthyroid1` imbalanced dataset to get 80 examples:

```{r example-pdfos, fig.width = 10, fig.height = 10}
library("imbalance")
data(newthyroid1)

newSamples <- pdfos(dataset = newthyroid1, numInstances = 80,
                    classAttr = "Class")
```

All of the algorithms can be used with the minimal parameters `dataset`, `numInstances` and `classAttr`, except for `wRACOG`, which does not have a `numInstances` parameter. The latter adjusts this number itself, and needs two datasets (more accurately, two partitions of the same dataset), `train` and `validation` to work.

The package also includes a method to plot a visual comparison between the oversampled dataset and the old
imbalanced dataset:

```{r example-plot, fig.width = 10, fig.height = 10}
# Bind a balanced dataset
newDataset <- rbind(newthyroid1, newSamples)
# Plot a visual comparison between new and old dataset
plotComparison(newthyroid1, newDataset, 
               attrs = names(newthyroid1)[1:3], 
               cols = 2, classAttr = "Class")
```

There is also a filtering example available, `neater`, which could be used with every oversampling method, either included in this package or in another one:

```{r example-neater, fig.width = 10, fig.height = 10}
filteredSamples <- neater(newthyroid1, newSamples, 
                          iterations = 500)
filteredNewDataset <- rbind(newthyroid1, filteredSamples)
plotComparison(newthyroid1, filteredNewDataset, 
               attrs = names(newthyroid1)[1:3])
```

# Oversampling
## MWMOTE
SMOTE is a classic algorithm which generates new examples by filling empty areas among the positive instances, it updates the training set iteratively, by perfoming:

\[E:=E\cup\{r+(y-x)\}, \quad x,y\in S^{+}, r\sim N(0,1)\]

It has a major setback though: it does not detect noisy instances. Therefore it can generate synthetic examples out of noisy ones or even between two minority classes, which if not cleansed up, may end up becoming noise inside a majority class cluster.

```{r, out.width="60%", fig.align='center', fig.cap='SMOTE generating noise', echo=FALSE, fig.pos="h"}
knitr::include_graphics("smote-flaws.png")
```

MWMOTE (*Majority Weighted Minority Oversampling Technique*) tries to overcome both problems. It intends to give more weight to borderline instances, small size minority cluster instances and examples near the borderline of the two clases. 

Let us recall the header of the method:

```
mwmote(dataset, numInstances, kNoisy, kMajority, kMinority,
       threshold, cmax, cclustering, classAttr)
```

A KNN algorithm will be used, where we call $d(x,y)$ the euclidean distance between $x$ and $y$. Let $NN^{k}(x)\subseteq S$ be the $k$-neighbourhood of $x$ among the whole trainning set (the $k$ closest instances with euclidean distance). Let $NN_{+}^k(x) \subseteq S^{+}$ be its $k$ minority neighbourhood and $NN_{-}^k(x) \subseteq S^{-}$ be its $k$ majority neighbourhood.

For ease of notation, we will name $k_1:=$`KNoisy`, $k_2:=$`KMajority`, $k_3:=$`KMinority`, $\alpha:=$`threshold`, $C:=$`clust`, $C_{clust}:=$`cclustering`.

We define $I_{\alpha,C}(x,y) = C_f(x,y) \cdot D_f(x,y)$, where if $x \notin NN_{+}^{k_3}(y)$ then $I_{\alpha,C}w(x,y) = 0$. 
Otherwise:
\[
  f(x) = \left\{\begin{array}{ll} 
                x &, x\le \alpha \\
                C & \textrm{en otro caso}
               \end{array}\right.,\qquad C_f(x,y) = \frac{C}{\alpha} \cdot f\left(\frac{d}{d(x,y)}\right)
\]

$C_f$ measures the closeness to $y$, that is, it will measure the closeness of borderline instances.

$D_f(x,y) = \frac{C_f(x,y)}{\sum_{z\in V} C_f(z,y)}$ will represent a density factor so an instance belonging 
to a compact cluster will have higher $\sum C_f(z,y)$ than another one belonging to a more sparse one.

Let $T_{clust}:= C_{clust} \cdot \frac{1}{|S_f^{+}|} \sum_{x\in S_f^{+}} \underset{y\in S_f^{+}, y\neq x}{min} d(x,y)$. We will also use a mean-average agglomerative hierarchical clustering of the minority instances with threshold $T_{clust}$, that is, we will use a mean distance:
\[dist(L_i, L_j) = \frac{1}{|L_i||L_j|} \sum_{x\in L_i} \sum_{y\in L_j} d(x,y)\]
and having started with a cluster per instance, and will proceed to join nearest clusters until minimum of distances is lower than $T_{clust}$.  

A general outline of the algorithm is:

* Firstly, MWMOTE computes a set of filtered positive instances: $S_f^{+}$, by erasing those instances whose $k_1$-neighborhood does not contain any positive instance.
* Secondly, it computes the positive boundary of $S_f^{+}$, that is, $U = \cup_{x \in S^{+}_f} NN_{-}^{k_2}(x)$ and the negative boundary by doing $V = \cup_{x \in U} NN_{+}^{k_3}(x)$.
* For each $x\in V$ compute probability of picking $x$ by assigning: $P(x) = \sum_{y\in U} I_{\alpha, C}(x,y)$ and normalizing those probabilities.
* Then, it calculates $L_1, \ldots, L_M$ clusters of $S^{+}$, with the aforementioned jerarquical agglomerative clustering algorithm and threshold $T_{clust}$.
* Generate `numInstances` examples by iteratively picking $x\in V$ with respect to probability $P(x)$, and updating $E:=E\cup \{x+r(y-x)\}$, where $y\in L_k$ uniformly picked and $L_k$ is the cluster containing $x$.


A few interesting considerations:

* Low $k_2$ is required in order to ensure we do not pick too many negative instances.
* For an opposite reason, a high $k_3$ must be selected to ensure we pick as many positive hard-to-learn borderline examples as we can.
* The higher the $C_{clust}$ parameter, the less and more-populated clusters we will get.

## RACOG and wRACOG
These set of algorithms assume that what we want to approximate is a discrete distribution $P(W_1, \ldots, W_d)$.

Computing that distribution can be too expensive, because we have to compute:
\[
  |\{\textrm{Posibles valores para }W_1\}| \cdots |\{\textrm{Posibles valores para} W_d\}|
\]
possible values.

We are going to approximate $P(W_1, \ldots, W_d)$ as $\prod_{i=1}^d P(W_i \mid W_{n(i)})$ where $n(i) \in 
\{1, \ldots, d\}$. Chow-Liu's algorithm \ref{alg:chowliu} will be used to meet that purpose. This algorithm,
minimizes Kullback-Leibler distance between two distributions:
\[
  D_{KL}(P \parallel Q) = \sum_{i} P(i) \left(\log P(i) - \log Q(i)\right)
\]

We recall the definition for the mutual information of two random discrete variables $W_i, W_j$:
\[
  I(W_i, W_j) = \sum_{w_1\in W_1} \sum_{w_2\in W_2} p(w_1, w_2) \log\left(\frac{p(w_1,w_2)}{p(w_1) p(w_2)}\right)
\]

The algorithm to approximate the distribution becomes:


```{r, out.width="60%", fig.align='center', fig.cap='Markov chain', echo=FALSE}
knitr::include_graphics("monte-carlo.png")
```

### RACOG
RACOG (\textit{Rapidly Converging Gibbs}) builds a Markov chain for each of the $m$ minority instances, 
ruling out the first $\beta$ generated instances and selecting a badge of synthetic examples each $\alpha$ iterations. That allows to lose dependence of previous values.

### wRACOG
RACOG has a problem: it depends on $\alpha, \beta$ and the number of instances requested. wRACOG (\textit{wrapper-based RACOG}) tries to overcome that problem. Let \textit{wrapper} be a classifier.
